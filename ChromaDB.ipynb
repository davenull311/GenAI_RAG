{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T13:46:08.320713Z",
     "start_time": "2023-10-22T13:46:08.318972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Ollama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "from langchain.agents import Tool, AgentExecutor\n",
    "# from langchain.agents import create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19aa4507398334c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:07.764179Z",
     "start_time": "2023-10-22T14:10:07.760696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the API key when using OpenAI embeddings\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516b0ea1eef2caa",
   "metadata": {},
   "source": [
    "# Load the  embeddings and the model\n",
    "For this notebook i will use the Mistral 7B model and the Ollama embeddings. You can also use the OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f873210b0f2bb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:13:11.219Z",
     "start_time": "2023-10-22T14:13:11.211783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ollama embeddings\n",
    "embeddings_open = OllamaEmbeddings(model = \"mistral\")\n",
    "\n",
    "# OpenAI embeddings\n",
    "# embedding = OpenAIEmbeddings()\n",
    "\n",
    "llm_open = Ollama(\n",
    "    model = \"mistral\",\n",
    "    # model = 'Llama2',\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004ca4c4196e842",
   "metadata": {},
   "source": [
    "# Load the data \n",
    "Load the data from the directory and split it into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381e27a6499e6d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:13.263732Z",
     "start_time": "2023-10-22T14:10:11.537487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Print number of txt files in directory\n",
    "# loader = DirectoryLoader('../data/langchain_doc_small', glob=\"./*.txt\")\n",
    "loader = DirectoryLoader('/Users/davenull311/Projects/GenAI/data/langchain_doc_small', glob = \"./*.txt\")\n",
    "\n",
    "\n",
    "# load pdfs from directory and print number of pdfs\n",
    "# loader = PyPDFLoader('../data/PDFs/How_to_build_your_carreer_in_AI.pdf')\n",
    "# loader = PyPDFLoader('/Users/davenull311/Projects/GenAI/data/PDFs/How_to_build_your_carreer_in_AI.pdf')\n",
    "\n",
    "\n",
    "# load another file directly\n",
    "# loader = DirectoryLoader('/your/path/to/file.txt')\n",
    "\n",
    "doc = loader.load ( )\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ffbcf0eae8b99",
   "metadata": {},
   "source": [
    "# Print the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0aad101a41700c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:13.265066Z",
     "start_time": "2023-10-22T14:10:13.263071Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aleph Alpha\\n\\nAleph Alpha# The Luminous series is a family of large language models. This example goes over how to use LangChain to interact with Aleph Alpha models\\n\\n# Install the package\\n\\n!pip install aleph\\n\\nalpha\\n\\nclient\\n\\n# create a new token: https://docs.aleph-alpha.com/docs/account/#create-a-new-token\\n\\nfrom getpass import getpass\\n\\nALEPH_ALPHA_API_KEY = getpass()\\n\\nfrom langchain.llms import AlephAlpha from langchain import PromptTemplate, LLMChain\\n\\ntemplate = \"\"\"Q: {question}\\n\\nA:\"\"\"\\n\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\n\\nllm = AlephAlpha(model=\"luminous\\n\\nextended\", maximum_tokens=20, stop_sequences=[\"Q:\"], aleph_alpha_api_key=ALEPH_ALPHA_API_KEY)\\n\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n\\nquestion = \"What is AI?\"\\n\\nllm_chain.run(question)\\n\\n\\' Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.\\\\n\\'\\n\\nprevious\\n\\nAI21\\n\\nnext\\n\\nAnyscale\\n\\nBy Harrison Chase\\n\\n© Copyright 2023, Harrison Chase.\\n\\nLast updated on Jun 13, 2023.' metadata={'source': '/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a5e0be0624637",
   "metadata": {},
   "source": [
    "# Split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fac36b7b5690f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:13.613910Z",
     "start_time": "2023-10-22T14:10:13.608962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500, \n",
    "    chunk_overlap = 50,\n",
    "    is_separator_regex = False)\n",
    "texts = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921794ad8962108",
   "metadata": {},
   "source": [
    "# Count the number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40c8b5d391da327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:14.484937Z",
     "start_time": "2023-10-22T14:10:14.480689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b6fb845bc037",
   "metadata": {},
   "source": [
    "# Print the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed4c59383cdf49c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:15.422234Z",
     "start_time": "2023-10-22T14:10:15.417849Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Aleph Alpha\\n\\nAleph Alpha# The Luminous series is a family of large language models. This example goes over how to use LangChain to interact with Aleph Alpha models\\n\\n# Install the package\\n\\n!pip install aleph\\n\\nalpha\\n\\nclient\\n\\n# create a new token: https://docs.aleph-alpha.com/docs/account/#create-a-new-token\\n\\nfrom getpass import getpass\\n\\nALEPH_ALPHA_API_KEY = getpass()\\n\\nfrom langchain.llms import AlephAlpha from langchain import PromptTemplate, LLMChain\\n\\ntemplate = \"\"\"Q: {question}\\n\\nA:\"\"\"', metadata={'source': '/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce407d89f1110a05",
   "metadata": {},
   "source": [
    "# Embed and store the texts\n",
    "Supplying a persist_directory will store the embeddings on disk, so that they can be loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0caf622e36bc7c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:20.077716Z",
     "start_time": "2023-10-22T14:10:17.208717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs done\n",
      "Texts done\n"
     ]
    }
   ],
   "source": [
    "# PDFs from directory\n",
    "# persist_directory = 'PDFs_How_to_build_your_carreer_in_AI'\n",
    "print ('PDFs done')\n",
    "\n",
    "# Langchain documentation\n",
    "persist_directory = '/Users/davenull311/Projects/GenAI/data/'\n",
    "print ('Texts done')\n",
    "\n",
    "# Your documents \n",
    "# persist_directory = 'your_new_database'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3a55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141ae84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb1 = Chroma.from_documents(texts[:5], embeddings_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b6dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "persist_directory = '/Users/davenull311/Projects/GenAI/data/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents = texts[:3], \n",
    "    embedding = embeddings_open, \n",
    "    persist_directory = persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b87a8f3b1ab1a",
   "metadata": {},
   "source": [
    "# Save to disc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1d92ce6d0af525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:21.985550Z",
     "start_time": "2023-10-22T14:10:21.980419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Persist the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da269fc5e9808fc4",
   "metadata": {},
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f012c910a6504b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:37.062806Z",
     "start_time": "2023-10-22T14:10:37.050960Z"
    }
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings_open\n",
    "    #embedding_function=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77acb94af62830",
   "metadata": {},
   "source": [
    "# Create the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccd5f8b671622d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:39.857550Z",
     "start_time": "2023-10-22T14:10:39.467642Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"What is this document about?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31727f127da2e2f1",
   "metadata": {},
   "source": [
    "# Print the number of documents that are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5b5b15ffee9a9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:40.542775Z",
     "start_time": "2023-10-22T14:10:40.537340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc7cfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='template = \"\"\"Q: {question}\\n\\nA:\"\"\"\\n\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\n\\nllm = AlephAlpha(model=\"luminous\\n\\nextended\", maximum_tokens=20, stop_sequences=[\"Q:\"], aleph_alpha_api_key=ALEPH_ALPHA_API_KEY)\\n\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n\\nquestion = \"What is AI?\"\\n\\nllm_chain.run(question)\\n\\n\\' Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.\\\\n\\'\\n\\nprevious\\n\\nAI21\\n\\nnext\\n\\nAnyscale', metadata={'source': '/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt'}),\n",
       " Document(page_content='Aleph Alpha\\n\\nAleph Alpha# The Luminous series is a family of large language models. This example goes over how to use LangChain to interact with Aleph Alpha models\\n\\n# Install the package\\n\\n!pip install aleph\\n\\nalpha\\n\\nclient\\n\\n# create a new token: https://docs.aleph-alpha.com/docs/account/#create-a-new-token\\n\\nfrom getpass import getpass\\n\\nALEPH_ALPHA_API_KEY = getpass()\\n\\nfrom langchain.llms import AlephAlpha from langchain import PromptTemplate, LLMChain\\n\\ntemplate = \"\"\"Q: {question}\\n\\nA:\"\"\"', metadata={'source': '/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt'}),\n",
       " Document(page_content='previous\\n\\nAI21\\n\\nnext\\n\\nAnyscale\\n\\nBy Harrison Chase\\n\\n© Copyright 2023, Harrison Chase.\\n\\nLast updated on Jun 13, 2023.', metadata={'source': '/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22f6c67728e4116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:41.439707Z",
     "start_time": "2023-10-22T14:10:41.435163Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745711b022aca6f",
   "metadata": {},
   "source": [
    "# Create the chain to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ff137a6d5dc18a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:42.349496Z",
     "start_time": "2023-10-22T14:10:42.345521Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_open,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=vectordb.as_retriever(),\n",
    "                                  return_source_documents=True,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c26eb8f42b29e1",
   "metadata": {},
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a93117b6c901b0fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:51.532368Z",
     "start_time": "2023-10-22T14:10:43.252111Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davenull311/anaconda3/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This document is about using the LangChain library with the Aleph Alpha Luminous extended model to interact with the Aleph Alpha API and ask questions. It includes instructions on how to install the necessary packages, create an API key, and use a PromptTemplate and LLMChain to run the query. The example question asked in this document is \"What is AI?\" and the expected answer is \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.\"\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " This document is about using the LangChain library with the Aleph Alpha Luminous extended model to interact with the Aleph Alpha API and ask questions. It includes instructions on how to install the necessary packages, create an API key, and use a PromptTemplate and LLMChain to run the query. The example question asked in this document is \"What is AI?\" and the expected answer is \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.\"\n",
      "\n",
      "\n",
      "Sources:\n",
      "/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt\n",
      "/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt\n",
      "/Users/davenull311/Projects/GenAI/data/langchain_doc_small/20_Aleph_Alpha_Aleph.txt\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "query = \"What is this document about?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b86c74c46570ef",
   "metadata": {},
   "source": [
    "# Create a prompt template to use in the chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75cee000aa10d601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:10:58.421712Z",
     "start_time": "2023-10-22T14:10:58.417481Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_prompt(template_num=\"template_1\"):\n",
    "    template = \"\"\" You are a helpful chatbot, named RSLT. You answer the questions of the customers giving a lot of details based on what you find in the context.\n",
    "Do not say anything that is not in the website\n",
    "You are to act as though you're having a conversation with a human.\n",
    "You are only able to answer questions, guide and assist, and provide recommendations to users. You cannot perform any other tasks outside of this.\n",
    "Your tone should be professional and friendly.\n",
    "Your purpose is to answer questions people might have, however if the question is unethical you can choose not to answer it.\n",
    "Your responses should always be one paragraph long or less.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    template2 = \"\"\"You are a helpful chatbot, named RSLT. You answer the questions of the customers giving a lot of details based on what you find in the context. \n",
    "    Your responses should always be one paragraph long or less.\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    if template_num == \"template_1\":\n",
    "        prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "        return prompt\n",
    "\n",
    "    elif template_num == \"template_2\":\n",
    "        prompt = PromptTemplate(input_variables=[\"question\"], template=template2)\n",
    "        return prompt\n",
    "\n",
    "    else:\n",
    "        print(\"Please choose a valid template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9408c72c79e80a",
   "metadata": {},
   "source": [
    "# Create the chain to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84d64a45a91aa4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:11:00.705541Z",
     "start_time": "2023-10-22T14:11:00.701934Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_open,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True,\n",
    "                                  verbose=True,\n",
    "                                  chain_type_kwargs={\"prompt\": build_prompt(\"template_1\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964035d669f039a2",
   "metadata": {},
   "source": [
    "# Question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "844c66124849bc0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:11:06.994282Z",
     "start_time": "2023-10-22T14:11:01.264850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "This document appears to be a guide for users who want to quickly get started with a specific software or system. The guide is divided into several sections, including Getting Started, Modules, Use Cases, Reference Docs, Ecosystem, and Additional Resources. It seems that the guide provides an overview of the features and functionalities of the software, as well as some practical examples of how to use it in different contexts. The author of the guide is Harrison Chase, and it was last updated on June 14, 2023.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This document appears to be a guide for users who want to quickly get started with a specific software or system. The guide is divided into several sections, including Getting Started, Modules, Use Cases, Reference Docs, Ecosystem, and Additional Resources. It seems that the guide provides an overview of the features and functionalities of the software, as well as some practical examples of how to use it in different contexts. The author of the guide is Harrison Chase, and it was last updated on June 14, 2023.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/2_Quickstart_Guide_Contents.txt\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/8_Getting_Started_Getting.txt\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/7_LLMs_LLMs_Note.txt\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/0_Welcome_to_LangChain.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"What is this document about?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6301e5032c792f",
   "metadata": {},
   "source": [
    "# Continue question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63197e0cca2ae5eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T14:11:16.561630Z",
     "start_time": "2023-10-22T14:11:11.121119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "LangChain is a framework for developing applications powered by language models. It enables developers to build data-aware and agentic applications that can call out to a language model, connect it to other sources of data, and allow it to interact with its environment. LangChain includes a variety of modules, use cases, reference documents, an ecosystem, and additional resources to support the development of these applications.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "LangChain is a framework for developing applications powered by language models. It enables developers to build data-aware and agentic applications that can call out to a language model, connect it to other sources of data, and allow it to interact with its environment. LangChain includes a variety of modules, use cases, reference documents, an ecosystem, and additional resources to support the development of these applications.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/0_Welcome_to_LangChain.txt\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/1_Welcome_to_LangChain.txt\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/1_Welcome_to_LangChain.txt\n",
      "/Users/erictak/PycharmProjects/freya/data/langchain_doc_small/0_Welcome_to_LangChain.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Lanchain?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd522fb30e2c4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
